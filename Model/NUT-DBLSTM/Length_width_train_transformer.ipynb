{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择在GPU或CPU上面运行\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10020\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# 拿到数据路径，方便后续读取\n",
    "dir_path = 'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\train'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "dataPaths = sorted(glob.glob(os.path.join(dir_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(dataPaths)\n",
    "\n",
    "# 遍历读取数据\n",
    "for dataPath in dataPaths:\n",
    "    # 读取数据\n",
    "    data_1 = pd.read_csv(dataPath)\n",
    "    \n",
    "    # 检查第一列的每一行，找到第一个大于0的值的位置N\n",
    "    for N in range(len(data_1)):\n",
    "        if data_1.iloc[N, 0] > 0:\n",
    "            break\n",
    "    \n",
    "    # 如果找到了大于0的值，则从第二列的第N行开始，往后取10020个点\n",
    "    if N < len(data_1):  # 确保N不会超出索引范围\n",
    "        data.append(data_1.iloc[N:N+M, 1])\n",
    "    \n",
    "    # 从文件名中提取x, y, z标签\n",
    "    filename = os.path.basename(dataPath)\n",
    "    # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "    parts = filename.split('_')\n",
    "    # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "    label1_str = parts[0]\n",
    "    # 提取最后一个\".csv\"之前的部分作为label2\n",
    "    label2_str = parts[1]\n",
    "    \n",
    "    # 将字符串标签转换为浮点数\n",
    "    label1 = float(label1_str)\n",
    "    label2 = float(label2_str)\n",
    "    \n",
    "    # 将两个标签值作为一个数组添加到labels列表中\n",
    "    labels.append([label1, label2])\n",
    "\n",
    "# 将数据和标签转换为numpy数组\n",
    "train_data = np.array(data, dtype=\"float\")\n",
    "train_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# 拿到数据路径，方便后续读取\n",
    "dir_path = 'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\validation'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "dataPaths = sorted(glob.glob(os.path.join(dir_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(dataPaths)\n",
    "\n",
    "# 遍历读取数据\n",
    "for dataPath in dataPaths:\n",
    "    # 读取数据\n",
    "    data_1 = pd.read_csv(dataPath)\n",
    "    \n",
    "    # 检查第一列的每一行，找到第一个大于0的值的位置N\n",
    "    for N in range(len(data_1)):\n",
    "        if data_1.iloc[N, 0] > 0:\n",
    "            break\n",
    "    \n",
    "    # 如果找到了大于0的值，则从第二列的第N行开始，往后取10020个点\n",
    "    if N < len(data_1):  # 确保N不会超出索引范围\n",
    "        data.append(data_1.iloc[N:N+M, 1])\n",
    "    \n",
    "    # 从文件名中提取x, y, z标签\n",
    "    filename = os.path.basename(dataPath)\n",
    "    # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "    parts = filename.split('_')\n",
    "    # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "    label1_str = parts[0]\n",
    "    # 提取最后一个\".csv\"之前的部分作为label2\n",
    "    label2_str = parts[1]\n",
    "    \n",
    "    # 将字符串标签转换为浮点数\n",
    "    label1 = float(label1_str)\n",
    "    label2 = float(label2_str)\n",
    "    \n",
    "    # 将两个标签值作为一个数组添加到labels列表中\n",
    "    labels.append([label1, label2])\n",
    "\n",
    "# 将数据和标签转换为numpy数组\n",
    "validation_data = np.array(data, dtype=\"float\")\n",
    "validation_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = train_data.reshape(-1,train_data.shape[1],1)\n",
    "print(train_datas.shape)\n",
    "validation_datas = validation_data.reshape(-1,validation_data.shape[1],1)\n",
    "print(validation_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.rcParams['font.family'] = ['Times New Roman']\n",
    "plt.plot(train_datas[1,:],linewidth=1.5)\n",
    "plt.xlabel('Sample point',fontdict={'weight': 'normal', 'size': 18})\n",
    "plt.ylabel('Amplitude(mm)',fontdict={'weight': 'normal', 'size': 18})\n",
    "#坐标轴刻度大小设置\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.xlim([0,10000])\n",
    "plt.savefig('D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\train_data.jpg', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCH = 6000\n",
    "learning_rate = 1e-4\n",
    "input_size=  10020   # 输入特征维度\n",
    "output_size = 2         # 输出维度\n",
    "\n",
    "max_seq_len = 1000         # 最大序列长度\n",
    "model_dim = 100\n",
    "num_heads = 4\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "#torch.from_numpy将 NumPy 数组转换为 PyTorch 张量\n",
    "#TensorDataset用于将张量数据和标签组合成一个数据集\n",
    "#DataLoader用于从数据集中加载批次数据，并进行训练或测试\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_datas),torch.from_numpy(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_datas),torch.from_numpy(validation_labels))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_size, model_dim, num_heads, num_layers, output_size, max_seq_len):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # 输入嵌入层\n",
    "        self.embedding = nn.Linear(input_size, model_dim)\n",
    "\n",
    "        # 位置编码\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, model_dim))\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=model_dim * 4  # 前馈网络的隐藏层维度\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(model_dim, model_dim // 2)\n",
    "        self.fc2 = nn.Linear(model_dim // 2, model_dim // 4)\n",
    "        self.fc3 = nn.Linear(model_dim // 4, output_size)\n",
    "        self.activate = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入嵌入\n",
    "        x = self.embedding(x)  # 形状: (batch_size, 1, model_dim)\n",
    "\n",
    "        # 添加位置编码\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Transformer 编码器前向传播\n",
    "        x = x.permute(1, 0, 2)  # 转换为 (1, batch_size, model_dim)\n",
    "        transformer_out = self.transformer_encoder(x)  # 形状: (1, batch_size, model_dim)\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)  # 转换回 (batch_size, 1, model_dim)\n",
    "\n",
    "        # 取最后一个时间步的输出\n",
    "        out1 = self.activate(transformer_out[:, -1, :])  # 形状: (batch_size, model_dim)\n",
    "\n",
    "        # 全连接层\n",
    "        out2 = self.fc1(out1)\n",
    "        out3 = self.fc2(out2)\n",
    "        out4 = self.fc3(out3)\n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型实例化\n",
    "model = TransformerPredictor(input_size, model_dim, num_heads, num_layers, output_size,max_seq_len)\n",
    "model.to(device)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#两个空列表用于存储训练和验证中的损失值\n",
    "train_loss_length_epoch = []\n",
    "train_loss_width_epoch = []\n",
    "\n",
    "#两个空列表用于存储训练和验证中的损失值\n",
    "validation_loss_length_epoch = []\n",
    "validation_loss_width_epoch = []\n",
    "\n",
    "A = 1\n",
    "B = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练和验证阶段\n",
    "for epoch in range(EPOCH):\n",
    "    if epoch % (EPOCH/10)==0:\n",
    "        print(\"-------第 {} 轮训练开始-------\".format(epoch+1)) \n",
    "\n",
    "    train_length_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "    train_width_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "\n",
    "    # 训练步骤开始\n",
    "    model.train() #在训练模式下，模型会计算并反向传播误差，并更新模型参数   \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() #在使用优化器更新参数之前，我们需要先将模型参数的梯度清零，\n",
    "                              #以避免之前的梯度对当前梯度的影响\n",
    "        \n",
    "        x1=x.type(torch.FloatTensor)\n",
    "        x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "        length1=y[:,0].type(torch.FloatTensor)\n",
    "        width1=y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "        x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "        length_hat = model(x3)[:,0]\n",
    "        width_hat=model(x3)[:,1]\n",
    "\n",
    "        train_loss_length = A* loss(length_hat, length2)\n",
    "        train_loss_width = B* loss(width_hat, width2)\n",
    "        \n",
    "        train_loss_length.backward()\n",
    "        train_loss_width.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_length_epoch += train_loss_length.item() * x3.size(0)\n",
    "        \n",
    "        train_width_epoch += train_loss_width.item() * x3.size(0) \n",
    "    #计算一个 epoch 内的平均训练损失\n",
    "    train_mean_length = train_length_epoch / len(train_loader.dataset)\n",
    "    #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "    train_loss_length_epoch.append([train_mean_length])\n",
    "\n",
    "    train_mean_width = train_width_epoch / len(train_loader.dataset)\n",
    "    train_loss_width_epoch.append([train_mean_width])\n",
    "\n",
    "    # 验证步骤开始\n",
    "    model.eval() \n",
    "    validation_length_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "    validation_width_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(validation_loader):\n",
    "            \n",
    "            x1=x.type(torch.FloatTensor)\n",
    "            x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "            length1 = y[:,0].type(torch.FloatTensor)\n",
    "            width1 = y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "            x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "            length_hat = model(x3)[:,0]\n",
    "            width_hat = model(x3)[:,1]\n",
    "\n",
    "            validation_loss_length = A* loss(length_hat, length2)\n",
    "            validation_loss_width = B* loss(width_hat, width2)\n",
    "            \n",
    "            validation_length_epoch += validation_loss_length.item() * x3.size(0)\n",
    "            \n",
    "            validation_width_epoch += validation_loss_width.item() * x3.size(0) \n",
    "        #计算一个 epoch 内的平均训练损失\n",
    "        validation_mean_length = validation_length_epoch / len(validation_loader.dataset)\n",
    "        #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "        validation_loss_length_epoch.append([validation_mean_length])\n",
    "\n",
    "        validation_mean_width = validation_width_epoch / len(validation_loader.dataset)\n",
    "        validation_loss_width_epoch.append([validation_mean_width])          \n",
    "\n",
    "    if epoch % (EPOCH/10) == 0:\n",
    "        print(f\"Epoch:{epoch}, Train_Length_Loss: {train_mean_length:.4f}, Train_Width_Loss: {train_mean_width:.4f},\\n\"\n",
    "      f\"Validation_Length_Loss: {validation_mean_length:.4f}, Validation_Width_Loss: {validation_mean_width:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\model_Length_width_transformer.pth') \n",
    "print(\"模型已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(10, 6)) # 创建Figure对象，并指定尺寸\n",
    "plt.rcParams['font.family'] = ['Times New Roman']\n",
    "\n",
    "# 创建第一个y轴\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(train_loss_length_epoch, 'r-', linewidth=2.5)\n",
    "ax1.plot(train_loss_length_epoch, marker='o', markersize=3, color='red', linestyle='None', label='Training loss for length')\n",
    "ax1.plot(train_loss_width_epoch, 'r-', linewidth=2.5)\n",
    "ax1.plot(train_loss_width_epoch, marker='*', markersize=3, color='red', linestyle='None', label='Training loss for width')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontdict={'weight': 'normal', 'size': 20})\n",
    "ax1.set_ylabel('Training loss', fontdict={'weight': 'normal', 'size': 20}, color='red')\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# 创建第二个y轴\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(validation_loss_length_epoch, 'b-', linewidth=2.5)\n",
    "ax1.plot(validation_loss_length_epoch, marker='o', markersize=3, color='blue', linestyle='None', label='Validation loss for length')\n",
    "ax1.plot(validation_loss_width_epoch, 'b-', linewidth=2.5)\n",
    "ax1.plot(validation_loss_width_epoch, marker='*', markersize=3, color='blue', linestyle='None', label='Validation loss for width')\n",
    "\n",
    "ax2.set_ylabel('Validation loss', fontdict={'weight': 'normal', 'size': 20}, color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# 设置x轴刻度\n",
    "epoch = np.arange(0,EPOCH+1,EPOCH/10)\n",
    "plt.xticks(epoch)\n",
    "\n",
    "# 添加图例\n",
    "ax1.legend(loc='upper left', fontsize=20)\n",
    "ax2.legend(loc='upper right', fontsize=20)\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig('D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\Model_Loss_transformer.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

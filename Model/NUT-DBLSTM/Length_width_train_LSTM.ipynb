{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择在GPU或CPU上面运行\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10020\n",
    "A = 0.1\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# 拿到数据路径，方便后续读取\n",
    "dir_path = 'E:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\train'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "dataPaths = sorted(glob.glob(os.path.join(dir_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(dataPaths)\n",
    "\n",
    "# 遍历读取数据\n",
    "for dataPath in dataPaths:\n",
    "    # 读取数据\n",
    "    data_1 = pd.read_csv(dataPath)\n",
    "    \n",
    "    # 检查第一列的每一行，找到第一个大于0的值的位置N\n",
    "    for N in range(len(data_1)):\n",
    "        if data_1.iloc[N, 1] != 0:\n",
    "            break\n",
    "    \n",
    "    # 如果找到了大于0的值，则从第二列的第N行开始，往后取10020个点\n",
    "    if N < len(data_1):  # 确保N不会超出索引范围\n",
    "        data.append(data_1.iloc[N:N+M, 1])\n",
    "    \n",
    "    # 从文件名中提取x, y, z标签\n",
    "    filename = os.path.basename(dataPath)\n",
    "    # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "    parts = filename.split('_')\n",
    "    # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "    label1_str = parts[0]\n",
    "    # 提取最后一个\".csv\"之前的部分作为label2\n",
    "    label2_str = parts[1]\n",
    "    \n",
    "    # 将字符串标签转换为浮点数\n",
    "    label1 = float(label1_str)\n",
    "    label2 = float(label2_str)\n",
    "    \n",
    "    # 将两个标签值作为一个数组添加到labels列表中\n",
    "    labels.append([label1, label2])\n",
    "\n",
    "# 将数据和标签转换为numpy数组\n",
    "train_data = np.array(data, dtype=\"float\")\n",
    "train_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# 拿到数据路径，方便后续读取\n",
    "dir_path = 'E:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\validation'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "dataPaths = sorted(glob.glob(os.path.join(dir_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(dataPaths)\n",
    "\n",
    "# 遍历读取数据\n",
    "for dataPath in dataPaths:\n",
    "    # 读取数据\n",
    "    data_1 = pd.read_csv(dataPath)\n",
    "    \n",
    "    # 检查第一列的每一行，找到第一个大于0的值的位置N\n",
    "    for N in range(len(data_1)):\n",
    "        if data_1.iloc[N, 1] != 0:\n",
    "            break\n",
    "    \n",
    "    # 如果找到了大于0的值，则从第二列的第N行开始，往后取10020个点\n",
    "    if N < len(data_1):  # 确保N不会超出索引范围\n",
    "        data.append(data_1.iloc[N:N+M, 1])\n",
    "    \n",
    "    # 从文件名中提取x, y, z标签\n",
    "    filename = os.path.basename(dataPath)\n",
    "    # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "    parts = filename.split('_')\n",
    "    # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "    label1_str = parts[0]\n",
    "    # 提取最后一个\".csv\"之前的部分作为label2\n",
    "    label2_str = parts[1]\n",
    "    \n",
    "    # 将字符串标签转换为浮点数\n",
    "    label1 = float(label1_str)\n",
    "    label2 = float(label2_str)\n",
    "    \n",
    "    # 将两个标签值作为一个数组添加到labels列表中\n",
    "    labels.append([label1, label2])\n",
    "\n",
    "# 将数据和标签转换为numpy数组\n",
    "validation_data = np.array(data, dtype=\"float\")\n",
    "validation_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = train_data.reshape(-1,train_data.shape[1],1)\n",
    "validation_datas= validation_data.reshape(-1,validation_data.shape[1],1)\n",
    "\n",
    "print(train_datas.shape)\n",
    "print(validation_datas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.rcParams['font.family'] = ['Times New Roman']\n",
    "plt.plot(train_datas[1],linewidth=1.5)\n",
    "plt.xlabel('Sample point',fontdict={'weight': 'normal', 'size': 18})\n",
    "plt.ylabel('Amplitude(mm)',fontdict={'weight': 'normal', 'size': 18})\n",
    "#坐标轴刻度大小设置\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.xlim([0,10000])\n",
    "plt.savefig('E:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\train_data.jpg', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "input_size = 10058\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "EPOCH = 6000\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "#torch.from_numpy将 NumPy 数组转换为 PyTorch 张量\n",
    "#TensorDataset用于将张量数据和标签组合成一个数据集\n",
    "#DataLoader用于从数据集中加载批次数据，并进行训练或测试\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_datas),torch.from_numpy(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_datas),torch.from_numpy(validation_labels))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//4)\n",
    "        self.fc3 = nn.Linear(hidden_size//4, output_size)\n",
    "        self.activate = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        wavelet = 'db4'\n",
    "        # 将张量移动至CPU上面\n",
    "        x1 = x.cpu()\n",
    "\n",
    "        # 将CPU张量转化为numpy\n",
    "        x2 = x1.numpy()\n",
    "        #离散小波分解\n",
    "        coeffs = pywt.wavedec(x2, wavelet, level=6)\n",
    "        xh1=coeffs[0]\n",
    "        xh2=coeffs[1]\n",
    "        xh3=coeffs[2]\n",
    "        xh4=coeffs[3]\n",
    "        xh5=coeffs[4]\n",
    "        xh6=coeffs[5]\n",
    "        xl6=coeffs[6]\n",
    "        # 连接分解后的序列\n",
    "        x3 = np.concatenate((xh1,xh2,xh3,xh4,xh5,xh6,xl6),axis=-1)\n",
    "\n",
    "        x4=torch.from_numpy(x3)\n",
    "        x5=x4.to(device)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers*2, x5.shape[0], self.hidden_size).to(x5.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x5.shape[0], self.hidden_size).to(x5.device)\n",
    "\n",
    "        out, _ = self.lstm(x5, (h0, c0))\n",
    "        out1 = self.activate(out)\n",
    "        out2 = self.fc1(out1[:, -1, :])\n",
    "        out4 = self.fc2(out2)\n",
    "        out5 = self.fc3(out4)\n",
    "        return out5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型实例化\n",
    "model = LSTMPredictor(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#两个空列表用于存储训练和验证中的损失值\n",
    "train_loss_length_epoch = []\n",
    "train_loss_width_epoch = []\n",
    "\n",
    "#两个空列表用于存储训练和验证中的损失值\n",
    "validation_loss_length_epoch = []\n",
    "validation_loss_width_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练和验证阶段\n",
    "for epoch in range(EPOCH):\n",
    "    if epoch % (EPOCH/10)==0:\n",
    "        print(\"-------第 {} 轮训练开始-------\".format(epoch+1)) \n",
    "\n",
    "    train_length_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "    train_width_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "\n",
    "    # 训练步骤开始\n",
    "    model.train() #在训练模式下，模型会计算并反向传播误差，并更新模型参数   \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() #在使用优化器更新参数之前，我们需要先将模型参数的梯度清零，\n",
    "                              #以避免之前的梯度对当前梯度的影响\n",
    "        \n",
    "        x1=x.type(torch.FloatTensor)\n",
    "        x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "        length1 = y[:,0].type(torch.FloatTensor)\n",
    "        width1 = y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "        x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "        length_hat = model(x3)[:,0]\n",
    "        width_hat=model(x3)[:,1]\n",
    "\n",
    "        train_loss_length = loss(length_hat.flatten(), length2.flatten())\n",
    "        train_loss_width = A* loss(width_hat.flatten(), width2.flatten())\n",
    "        \n",
    "        train_loss_length.backward()\n",
    "        train_loss_width.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_length_epoch += train_loss_length.item() * x3.size(0)\n",
    "        \n",
    "        train_width_epoch += train_loss_width.item() * x3.size(0) \n",
    "    #计算一个 epoch 内的平均训练损失\n",
    "    train_mean_length = train_length_epoch / len(train_loader.dataset)\n",
    "    #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "    train_loss_length_epoch.append([train_mean_length])\n",
    "\n",
    "    train_mean_width = train_width_epoch / len(train_loader.dataset)\n",
    "    train_loss_width_epoch.append([train_mean_width])\n",
    "\n",
    "    # 验证步骤开始\n",
    "    model.eval() \n",
    "    validation_length_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "    validation_width_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(validation_loader):\n",
    "            \n",
    "            x1=x.type(torch.FloatTensor)\n",
    "            x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "            length1 = y[:,0].type(torch.FloatTensor)\n",
    "            width1 = y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "            x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "            length_hat = model(x3)[:,0]\n",
    "            width_hat = model(x3)[:,1]\n",
    "\n",
    "            validation_loss_length = loss(length_hat.flatten(), length2.flatten())\n",
    "            validation_loss_width = A* loss(width_hat.flatten(), width2.flatten())\n",
    "            \n",
    "            validation_length_epoch += validation_loss_length.item() * x3.size(0)\n",
    "            \n",
    "            validation_width_epoch += validation_loss_width.item() * x3.size(0) \n",
    "        #计算一个 epoch 内的平均训练损失\n",
    "        validation_mean_length = validation_length_epoch / len(validation_loader.dataset)\n",
    "        #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "        validation_loss_length_epoch.append([validation_mean_length])\n",
    "\n",
    "        validation_mean_width = validation_width_epoch / len(validation_loader.dataset)\n",
    "        validation_loss_width_epoch.append([validation_mean_width])          \n",
    "\n",
    "    if epoch % (EPOCH/10) == 0:\n",
    "        print(f\"Epoch:{epoch}, Train_Length_Loss: {train_mean_length:.4f}, Train_Width_Loss: {train_mean_width:.4f},\\n\"\n",
    "      f\"Validation_Length_Loss: {validation_mean_length:.4f}, Validation_Width_Loss: {validation_mean_width:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'E:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\model_Length_width_LSTM.pth') \n",
    "print(\"模型已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(12, 8)) # 创建Figure对象，并指定尺寸\n",
    "plt.rcParams['font.family'] = ['Times New Roman']\n",
    "\n",
    "# 创建第一个y轴\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(train_loss_length_epoch, 'r-', linewidth=2.5)\n",
    "ax1.plot(train_loss_length_epoch, marker='o', markersize=5, color='red', linestyle='None', label='Training loss for length')\n",
    "ax1.plot(train_loss_width_epoch, 'r-', linewidth=2.5)\n",
    "ax1.plot(train_loss_width_epoch, marker='*', markersize=5, color='red', linestyle='None', label='Training loss for width')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontdict={'weight': 'normal', 'size': 20})\n",
    "ax1.set_ylabel('Training loss', fontdict={'weight': 'normal', 'size': 20}, color='red')\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# 创建第二个y轴\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(validation_loss_length_epoch, 'b-', linewidth=2.5)\n",
    "ax1.plot(validation_loss_length_epoch, marker='o', markersize=5, color='blue', linestyle='None', label='Validation loss for length')\n",
    "ax1.plot(validation_loss_width_epoch, 'b-', linewidth=2.5)\n",
    "ax1.plot(validation_loss_width_epoch, marker='*', markersize=5, color='blue', linestyle='None', label='Validation loss for width')\n",
    "\n",
    "ax2.set_ylabel('Validation loss', fontdict={'weight': 'normal', 'size': 20}, color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# 设置x轴刻度\n",
    "epoch = np.arange(0, EPOCH + 1, EPOCH // 10)\n",
    "plt.xticks(epoch)\n",
    "\n",
    "# 添加图例\n",
    "ax1.legend(loc='upper left', fontsize=20)\n",
    "ax2.legend(loc='upper right', fontsize=20)\n",
    "\n",
    "# 创建放大图的子图（inset）\n",
    "x1 = int(EPOCH*0.7)\n",
    "x2 = int(EPOCH)\n",
    "\n",
    "ax_inset = plt.axes([0.45, 0.25, 0.4, 0.4])  # [left, bottom, width, height]\n",
    "ax_inset.plot(train_loss_length_epoch, 'r-', linewidth=2.5)\n",
    "ax_inset.plot(train_loss_length_epoch, marker='o', markersize=5, color='red', linestyle='None', label='Training loss for length')\n",
    "ax_inset.plot(train_loss_width_epoch, 'r-', linewidth=2.5)\n",
    "ax_inset.plot(train_loss_width_epoch, marker='*', markersize=5, color='red', linestyle='None', label='Training loss for width')\n",
    "\n",
    "ax_inset.plot(validation_loss_length_epoch, 'b-', linewidth=2.5)\n",
    "ax_inset.plot(validation_loss_length_epoch, marker='o', markersize=5, color='blue', linestyle='None', label='Validation loss for length')\n",
    "ax_inset.plot(validation_loss_width_epoch, 'b-', linewidth=2.5)\n",
    "ax_inset.plot(validation_loss_width_epoch, marker='*', markersize=5, color='blue', linestyle='None', label='Validation loss for width')\n",
    "\n",
    "max_val = max(\n",
    "    max(train_loss_length_epoch[x1:x2]),\n",
    "    max(train_loss_width_epoch[x1:x2]),\n",
    "    max(validation_loss_length_epoch[x1:x2]),\n",
    "    max(validation_loss_width_epoch[x1:x2])\n",
    ")\n",
    "\n",
    "ax_inset.set_xlim(x1, x2)\n",
    "ax_inset.set_ylim(0, max_val[0]*1.1)\n",
    "\n",
    "ax_inset.tick_params(axis='both', which='major', labelsize=10)\n",
    "# 保存图像\n",
    "plt.savefig('E:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\NUT-DBLSTM\\\\Model_Loss_LSTM.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

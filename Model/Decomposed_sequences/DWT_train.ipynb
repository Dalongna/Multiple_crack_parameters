{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择在GPU或CPU上面运行\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算FFT\n",
    "def calculate_fft(signal, sampling_rate):\n",
    "    N = len(signal)  # 信号长度\n",
    "    fft_values = np.fft.fft(signal)  # 计算FFT\n",
    "    fft_magnitude = np.abs(fft_values)  # 取模\n",
    "    fft_magnitude = fft_magnitude[:N // 2]  # 只取一半（正频率部分）\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)[:N // 2]  # 频率轴\n",
    "    return freq, fft_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10020\n",
    "BATCH_SIZE = 20\n",
    "input_size = M\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "EPOCH = 6000\n",
    "learning_rate = 1e-3\n",
    "A = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//4)\n",
    "        self.fc3 = nn.Linear(hidden_size//4, output_size)\n",
    "        self.activate = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out1 = self.activate(out)\n",
    "        out2 = self.fc1(out1[:, -1, :])\n",
    "        out3 = self.fc2(out2)\n",
    "        out4 = self.fc3(out3)\n",
    "        return out4\n",
    "#模型实例化\n",
    "model = LSTMPredictor(input_size, hidden_size, num_layers, output_size)\n",
    "model.to(device)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿到数据路径，方便后续读取\n",
    "train_path = 'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\DWT_Reconstruction_train'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "train_dataPaths = sorted(glob.glob(os.path.join(train_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(train_dataPaths)\n",
    "\n",
    "# 拿到数据路径，方便后续读取\n",
    "validation_path = 'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Data\\\\DWT_Reconstruction_validation'\n",
    "# 获取目录及其子目录下所有CSV文件的路径\n",
    "validation_dataPaths = sorted(glob.glob(os.path.join(validation_path, '**', '*.csv'), recursive=True))\n",
    "random.seed(42)\n",
    "random.shuffle(validation_dataPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [5]\n",
    "for k in n:\n",
    "    print(k)\n",
    "    if k == 7:\n",
    "        wavelet = \"A6\"\n",
    "    else:\n",
    "        wavelet = f\"D{k}\"\n",
    "    print(f'wavelet:{wavelet}')\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    # 遍历读取数据\n",
    "    for dataPath in train_dataPaths:\n",
    "        \n",
    "        # 读取数据\n",
    "        data_1 = pd.read_csv(dataPath)\n",
    "        \n",
    "        data.append(data_1.iloc[0:M, k-1])\n",
    "        \n",
    "        # 从文件名中提取x, y, z标签\n",
    "        filename = os.path.basename(dataPath)\n",
    "        # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "        parts = filename.split('_')\n",
    "        # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "        label1_str = parts[0]\n",
    "        # 提取最后一个\".csv\"之前的部分作为label2\n",
    "        label2_str = parts[1]\n",
    "        \n",
    "        # 将字符串标签转换为浮点数\n",
    "        label1 = float(label1_str)\n",
    "        label2 = float(label2_str)\n",
    "        \n",
    "        # 将两个标签值作为一个数组添加到labels列表中\n",
    "        labels.append([label1, label2])\n",
    "\n",
    "    # 将数据和标签转换为numpy数组\n",
    "    train_data = np.array(data, dtype=\"float\")\n",
    "    train_labels = np.array(labels)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 遍历读取数据\n",
    "    for dataPath in validation_dataPaths:\n",
    "        # 读取数据\n",
    "        data_1 = pd.read_csv(dataPath)\n",
    "        \n",
    "        data.append(data_1.iloc[0:M, k-1])\n",
    "        \n",
    "        # 从文件名中提取x, y, z标签\n",
    "        filename = os.path.basename(dataPath)\n",
    "        # 假设文件名格式为 \"some_prefix_label1_label2.csv\"\n",
    "        parts = filename.split('_')\n",
    "        # 提取最后一个\"_\"之前的部分作为prefix，之后的作为label1\n",
    "        label1_str = parts[0]\n",
    "        # 提取最后一个\".csv\"之前的部分作为label2\n",
    "        label2_str = parts[1]\n",
    "        \n",
    "        # 将字符串标签转换为浮点数\n",
    "        label1 = float(label1_str)\n",
    "        label2 = float(label2_str)\n",
    "        \n",
    "        # 将两个标签值作为一个数组添加到labels列表中\n",
    "        labels.append([label1, label2])\n",
    "\n",
    "    # 将数据和标签转换为numpy数组\n",
    "    validation_data = np.array(data, dtype=\"float\")\n",
    "    validation_labels = np.array(labels)\n",
    "\n",
    "    train_datas=train_data.reshape(-1,train_data.shape[1],1)\n",
    "    print(f'train_datas.shape:{train_datas.shape}')\n",
    "\n",
    "    validation_datas=validation_data.reshape(-1,validation_data.shape[1],1)\n",
    "    print(f'validation_datas.shape:{validation_datas.shape}')\n",
    "\n",
    "    #时域信号绘制\n",
    "    plt.style.use('default')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.rcParams['font.family'] = ['Times New Roman']\n",
    "    plt.plot(train_datas[1,:],linewidth=1.5)\n",
    "    plt.xlabel('Sample point',fontdict={'weight': 'normal', 'size': 18})\n",
    "    plt.ylabel('Amplitude(mm)',fontdict={'weight': 'normal', 'size': 18})\n",
    "    #坐标轴刻度大小设置\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.xlim([0,M])\n",
    "    plt.savefig(f'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\Decomposed_sequences\\\\train_data_TIME_{wavelet}.jpg', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    #频域信号绘制\n",
    "    sampling_rate = 1e7\n",
    "    freq, fft_magnitude = calculate_fft(train_datas[1,:], sampling_rate)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.rcParams['font.family'] = ['Times New Roman']\n",
    "    plt.plot(freq,fft_magnitude,linewidth=1.5)\n",
    "    plt.xlabel('Sample point',fontdict={'weight': 'normal', 'size': 18})\n",
    "    plt.ylabel('FFT Amplitude',fontdict={'weight': 'normal', 'size': 18})\n",
    "    #坐标轴刻度大小设置\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    # plt.xlim([0,M])\n",
    "    plt.savefig(f'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\Decomposed_sequences\\\\train_data_FFT_{wavelet}.jpg', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    # 准备数据\n",
    "    #torch.from_numpy将 NumPy 数组转换为 PyTorch 张量\n",
    "    #TensorDataset用于将张量数据和标签组合成一个数据集\n",
    "    #DataLoader用于从数据集中加载批次数据，并进行训练或测试\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_datas),torch.from_numpy(train_labels))\n",
    "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "    validation_dataset = TensorDataset(torch.from_numpy(validation_datas),torch.from_numpy(validation_labels))\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size= BATCH_SIZE, shuffle = False)\n",
    "\n",
    "    #两个空列表用于存储训练和验证中的损失值\n",
    "    train_loss_length_epoch=[]\n",
    "    train_loss_width_epoch=[]\n",
    "\n",
    "    #两个空列表用于存储训练和验证中的损失值\n",
    "    valid_loss_length_epoch=[]\n",
    "    valid_loss_width_epoch=[]\n",
    "\n",
    "    #训练和验证阶段\n",
    "    for epoch in range(EPOCH):\n",
    "        if epoch % (EPOCH/10)==0:\n",
    "            print(\"-------第 {} 轮训练开始-------\".format(epoch+1)) \n",
    "\n",
    "        train_length_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "        train_width_epoch = 0.0 #这段代码放在epoch循环中，每次循环时清零\n",
    "\n",
    "        # 训练步骤开始\n",
    "        model.train() #在训练模式下，模型会计算并反向传播误差，并更新模型参数   \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() #在使用优化器更新参数之前，我们需要先将模型参数的梯度清零，\n",
    "                                #以避免之前的梯度对当前梯度的影响\n",
    "            \n",
    "            x1=x.type(torch.FloatTensor)\n",
    "            x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "            length1=y[:,0].type(torch.FloatTensor)\n",
    "            width1=y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "            x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "            length_hat = model(x3)[:,0]\n",
    "            width_hat=model(x3)[:,1]\n",
    "\n",
    "            train_loss_length =  loss(length_hat, length2)\n",
    "            train_loss_width =  A*loss(width_hat, width2)\n",
    "            \n",
    "            train_loss_length.backward()\n",
    "            train_loss_width.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_length_epoch += train_loss_length.item() * x3.size(0)\n",
    "            \n",
    "            train_width_epoch += train_loss_width.item() * x3.size(0) \n",
    "        #计算一个 epoch 内的平均训练损失\n",
    "        train_mean_length = train_length_epoch / len(train_loader.dataset)\n",
    "        #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "        train_loss_length_epoch.append([train_mean_length])\n",
    "\n",
    "        train_mean_width = train_width_epoch / len(train_loader.dataset)\n",
    "        train_loss_width_epoch.append([train_mean_width])\n",
    "\n",
    "        model.eval()\n",
    "        valid_length_epoch = 0.0\n",
    "        valid_width_epoch = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (x_data, y_label) in enumerate(validation_loader):\n",
    "                \n",
    "                x1=x.type(torch.FloatTensor)\n",
    "                x2 = x1.permute(0,2,1) #将x1的维度进行调换，该例中将第1个维度保持不变，第2个维度和第3个进行交换\n",
    "\n",
    "                length1=y[:,0].type(torch.FloatTensor)\n",
    "                width1=y[:,1].type(torch.FloatTensor)\n",
    "\n",
    "                x3, length2,width2 = x2.to(device), length1.to(device),width1.to(device)\n",
    "\n",
    "                length_hat = model(x3)[:,0]\n",
    "                width_hat=model(x3)[:,1]\n",
    "\n",
    "                valid_loss_length = loss(length_hat, length2)\n",
    "                valid_loss_width = A*loss(width_hat, width2)\n",
    "\n",
    "                valid_length_epoch += valid_loss_length.item() * x3.size(0)\n",
    "            \n",
    "                valid_width_epoch += valid_loss_width.item() * x3.size(0) \n",
    "\n",
    "            valid_mean_length = valid_length_epoch / len(validation_loader.dataset)\n",
    "            #将平均训练损失 train_mean_loss 添加到 train_loss_mean 列表中\n",
    "            valid_loss_length_epoch.append([valid_mean_length])\n",
    "\n",
    "            valid_mean_width = valid_width_epoch / len(validation_loader.dataset)\n",
    "            valid_loss_width_epoch.append([valid_mean_width])\n",
    "\n",
    "        if epoch % (EPOCH/10) == 0:\n",
    "            print(f\"Epoch:{epoch}, Train_Length_Loss: {train_mean_length:.4f}, Train_Width_Loss: {train_mean_width:.4f}\")   \n",
    "            print(f\"Epoch:{epoch}, Valid_Length_Loss: {valid_mean_length:.4f}, Valid_Width_Loss: {valid_mean_width:.4f}\")   \n",
    "\n",
    "    torch.save(model, f'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\Decomposed_sequences\\\\model_Length_width_{wavelet}.pth') \n",
    "    print(f\"Model for {wavelet} has been saved\")\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.figure(figsize=(10, 6)) # 创建Figure对象，并指定尺寸\n",
    "    plt.rcParams['font.family'] = ['Times New Roman']\n",
    "\n",
    "    epoch = np.arange(0, EPOCH+10, EPOCH/10)\n",
    "\n",
    "    # 创建第一个y轴\n",
    "    ax1 = plt.gca()\n",
    "    ax1.plot(train_loss_length_epoch, 'r-', linewidth=2.5)\n",
    "    ax1.plot(train_loss_length_epoch, marker='o', markersize=3, color='red', linestyle='None', label='Training loss for length')\n",
    "    ax1.plot(valid_loss_length_epoch, 'r-', linewidth=2.5)\n",
    "    ax1.plot(valid_loss_length_epoch, marker='o', markersize=3, color='black', linestyle='None', label='Validation loss for length')\n",
    "\n",
    "    ax1.set_xlabel('Epoch', fontdict={'weight': 'normal', 'size': 18})\n",
    "    ax1.set_ylabel('Loss for length', fontdict={'weight': 'normal', 'size': 18}, color='red')\n",
    "    ax1.tick_params(axis='y', labelcolor='red')\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # 创建第二个y轴\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(train_loss_width_epoch, 'b-', linewidth=2.5)\n",
    "    ax2.plot(train_loss_width_epoch, marker='o', markersize=3, color='blue', linestyle='None', label='Training loss for width')\n",
    "    ax2.plot(valid_loss_width_epoch, 'b-', linewidth=2.5)\n",
    "    ax2.plot(valid_loss_width_epoch, marker='o', markersize=3, color='green', linestyle='None', label='Validation loss for width')\n",
    "\n",
    "    ax2.set_ylabel('Loss for width', fontdict={'weight': 'normal', 'size': 18}, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # 设置x轴刻度\n",
    "    plt.xticks(epoch)\n",
    "\n",
    "    # 添加图例\n",
    "    ax1.legend(loc='upper left', fontsize=20)\n",
    "    ax2.legend(loc='upper right', fontsize=20)\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig(f'D:\\\\Desktop\\\\Python\\\\Length_width\\\\Model\\\\Decomposed_sequences\\\\Model_Loss_{wavelet}.jpg', dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
